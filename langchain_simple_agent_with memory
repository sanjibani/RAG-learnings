pip install -u langgraph langchain-tavily langgraph-checkpoint-sqlite
pip install --user langgraph langchain-tavily langgraph-checkpoint-sqlite
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="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"
import os
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"]  = "****"

os.environ["TAVILY_API_KEY"] = "****"

from langchain_tavily import TavilySearch
from langchain_community.tools.tavily_search import TavilySearchResults

tool = TavilySearchResults(
    api_key="********",   # or set TAVILY_API_KEY env var
    max_results=2
)
search_result = tool.invoke("what's the weather in sf")
print(search_result)

pip install --upgrade "tavily-python>=0.3.0" "langchain-community>=0.0.29"
from langchain_community.tools.tavily_search import TavilySearchResults

tool = TavilySearchResults(
    api_key="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",
    max_results=2
)
result = tool.invoke("what's the weather in sf")
print(result)

pip install --upgrade "langchain-tavily>=0.1"
from langchain_tavily import TavilySearch

tool = TavilySearch(
    tavily_api_key="****************",
    max_results=2
)

result = tool.invoke("what's the weather in SF")
print(result)

import getpass
import os

if not os.environ.get("ANTHROPIC_API_KEY"):
  os.environ["ANTHROPIC_API_KEY"] = getpass.getpass("Enter API key for Anthropic: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("claude-3-5-sonnet-latest", model_provider="anthropic")

pip install -U langchain-anthropic
import getpass
import os

if not os.environ.get("ANTHROPIC_API_KEY"):
  os.environ["ANTHROPIC_API_KEY"] = getpass.getpass("Enter API key for Anthropic: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("claude-3-5-sonnet-latest", model_provider="anthropic")
query = "Hi!"
response = model.invoke([{"role": "user", "content": query}])

response.text()
model_with_tools = model.bind_tools()
tools = [search]

tools = [tool]
model_with_tools = model.bind_tools(tools)
query = "hi"
response =  model_with_tools.invoke([{"role":"user" , "content":query}])
print({response.text()})
print({response.tool_calls})
print({response.tool_calls})

print(f"Tool calls: {response.tool_calls}")
print(response)
query = "what's the current weather like in Hanoi, Vietnam"
response = model_with_tools.invoke([{"role":"user", "content": query}])

print(response.text())
print(response.tool_calls)
search current weather weather reports

print(f"Tool calls: {response.tool_calls}")

from langgraph.prebuilt import create_react_agent
agent_executor = create_react_agent(model, tools)
input_message = {"role":"user" , "content": "hi"}
agent_response = agent_executor.invoke({"messages": [input_message]})
print(agent_response)
for(res in agent_response){}
for(res in agent_response['messages']){
    print(res)
}
for res in agent_response['messages']
for res in agent_response['messages']:
    res.pretty_print()
input_message = {"role":"user" , "content": "what's the weather in Hanoi is like now"}
agent_response = agent_executor.invoke({"messages": [input_message]})
for res in agent_response['messages']:
    res.pretty_print()
for step in agent_executor.stream({"messages": [input_message]}, stream_mode="values"):
    step["messages"][-1].pretty_print()
for step, metadata in agent_executor.stream(
    {"messages": [input_message]}, stream_mode="messages"
):
    if metadata["langgraph_node"] == "agent" and (text := step.text()):
        print(text, end="|")

// agent memory implementation
from langgraph.checkpoint.memory import MemorySaver
memory = MemorySaver()
agent_executor = create_react_agent(model , tools , checkpointer = memory)
config = { "configurable": {"thread_id": "12345"}}
input_messages = {"role":"user" , "content": "hi , my name is Sanjib"}
for step in agent_executor.stream(
    {"messages":[input_messages]} , config , stream_mode="values"
):
 step["messages"][-1].pretty_print()
input_messages = {"role":"user" , "content": "what's my name"}

for step in agent_executor.stream(
    {"messages":[input_messages]} , config , stream_mode="values"
):
 step["messages"][-1].pretty_print()
